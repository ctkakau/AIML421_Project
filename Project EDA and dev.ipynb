{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bef1de",
   "metadata": {},
   "source": [
    "#AIML421 Project\n",
    "\n",
    "Train a model to classify pictures into one of three classes ('cherry', 'strawberry', 'tomato').  The model is built with the pytorch library, trains on data in one folder, outputs model parameters as a file.  The saved model is called by a second python script which classifies unseen data in a folder 'testdata', so it is important to test this part of the code as well.  \n",
    "\n",
    "Model design and training can be done in colab with preprocessing and training in one script (train.py), trained model parameters in another script (model.pth) and a third script holding the test procedure (test.py)\n",
    "1. conduct Exploratory Data Analysis\n",
    "2. determine pre-processing\n",
    "3. establish baseline model\n",
    "4. buil CNN \n",
    "5. tune the CNN model:  \n",
    "6. report\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "### Data preparation for analysis and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01abd599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.io as tvio\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600065a",
   "metadata": {},
   "source": [
    "Data consists of around 4500 JPG images split into three classes:\n",
    " - 'cherry', 'tomato', 'strawberry'\n",
    "Classes are well balanced with around 1500 instances of each class.  \n",
    "Most of the JPG images are 300x300 pixel, colour images, in a range of different contexts (i.e. against different backgrounds, from different angles and with different lighting).  \n",
    "The JPG images can be converted to tensors with dimensions [3, 300, 300] representing:\n",
    "  - three channels, one for each of Red, Green, Blue colour vectors\n",
    "  - 300 pixels in width, each pixel with integer value between 0:255\n",
    "  - 300 pixels in height, each pixel with integer value between 0:255\n",
    "\n",
    "## Preprocessing requirements  \n",
    "### Transformations  \n",
    "#### image size\n",
    "Not all images are 300 x 300 pixels, which can cause issues for matrix multiplication functions within the convolutional neural network (i.e. matrices with incompatible dimensions will raise an error).  Treatment:  enforce image size at 300x300.  This will introduce some distortion, as data is either added to the tensor in order to fill the image size up to 300x300, or data is removed from the tensor to reduce the image size to 300x300.  \n",
    "\n",
    "#### normalisation\n",
    "We will normalise the dataset to reduce the effects of extreme values.  \n",
    "\n",
    "#### colour, shape, texture changes\n",
    "The classes share some characteristics, in particular:\n",
    "  - all are fruits\n",
    "  - all are red, with green stalk\n",
    "  - cherries and tomatoes are round have smooth skin\n",
    "\n",
    "So we may also try transforming the images to black and white images, in order to reduce the potentially confusing effect of the colour similarity.  \n",
    "We may also try shape distorting transformations, in order to reduce the potentially confusing effect of the shape and texture similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eadda05",
   "metadata": {},
   "source": [
    "### Baseline settings \n",
    "#### Transformations  \n",
    "For all models we apply, at a minimum, the following functional transformations:\n",
    "  - resize image to 300x300\n",
    "  - normalise to mean: 0.5, standard deviation: 0.5\n",
    "  - transform to a pytorch tensor\n",
    "\n",
    "#### Initial model - multilayer cnn  \n",
    "The initial model:\n",
    "Batch size: 4  \n",
    "Epochs: 10  \n",
    "Training set: approx 2500 images  \n",
    "Test set:  approx 1800 images  \n",
    "\n",
    "Model configuration:  \n",
    "2D Convolutional layer (channels in = 3, channels out = 6, kernel size = 5)  \n",
    "2D Max pooling layer (size = 2, stride = 2)  \n",
    "2D Convolutional layer (channels in = 3, channels out = 6, kernel size = 5)  \n",
    "2D Max pooling layer (size = 2, stride = 2)  \n",
    "Linear layer (input size = (4, 82944), output size = 120)  \n",
    "Linear layer (input size = 120, output size = 84)  \n",
    "Linear layer (input size = 84, output size = 3)  \n",
    "\n",
    "All layers except final layer, use ReLU activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121c504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:   ('cherry', 'strawberry', 'tomato')\n",
      "Net() class defined\n",
      "net instantiated\n",
      "loss and optimizer functions defined\n",
      "\n",
      "============\n",
      "epoch:  0\n",
      "[1,    20] loss: 0.011\n",
      "[1,    40] loss: 0.011\n",
      "[1,    60] loss: 0.011\n",
      "[1,    80] loss: 0.011\n",
      "[1,   100] loss: 0.011\n",
      "[1,   120] loss: 0.011\n",
      "[1,   140] loss: 0.011\n",
      "[1,   160] loss: 0.011\n",
      "[1,   180] loss: 0.011\n",
      "[1,   200] loss: 0.011\n",
      "[1,   220] loss: 0.011\n",
      "[1,   240] loss: 0.011\n",
      "[1,   260] loss: 0.011\n",
      "[1,   280] loss: 0.010\n",
      "[1,   300] loss: 0.011\n",
      "[1,   320] loss: 0.011\n",
      "[1,   340] loss: 0.011\n",
      "[1,   360] loss: 0.011\n",
      "[1,   380] loss: 0.011\n",
      "[1,   400] loss: 0.011\n",
      "[1,   420] loss: 0.011\n",
      "[1,   440] loss: 0.011\n",
      "[1,   460] loss: 0.011\n",
      "[1,   480] loss: 0.010\n",
      "[1,   500] loss: 0.010\n",
      "[1,   520] loss: 0.011\n",
      "[1,   540] loss: 0.011\n",
      "[1,   560] loss: 0.010\n",
      "[1,   580] loss: 0.010\n",
      "[1,   600] loss: 0.010\n",
      "\n",
      "============\n",
      "epoch:  1\n",
      "[2,    20] loss: 0.010\n",
      "[2,    40] loss: 0.010\n",
      "[2,    60] loss: 0.010\n",
      "[2,    80] loss: 0.011\n",
      "[2,   100] loss: 0.010\n",
      "[2,   120] loss: 0.010\n",
      "[2,   140] loss: 0.010\n",
      "[2,   160] loss: 0.010\n",
      "[2,   180] loss: 0.010\n",
      "[2,   200] loss: 0.010\n",
      "[2,   220] loss: 0.010\n",
      "[2,   240] loss: 0.010\n",
      "[2,   260] loss: 0.010\n",
      "[2,   280] loss: 0.010\n",
      "[2,   300] loss: 0.010\n",
      "[2,   320] loss: 0.011\n",
      "[2,   340] loss: 0.010\n",
      "[2,   360] loss: 0.010\n",
      "[2,   380] loss: 0.010\n",
      "[2,   400] loss: 0.009\n",
      "[2,   420] loss: 0.009\n",
      "[2,   440] loss: 0.010\n",
      "[2,   460] loss: 0.009\n",
      "[2,   480] loss: 0.011\n",
      "[2,   500] loss: 0.010\n",
      "[2,   520] loss: 0.010\n",
      "[2,   540] loss: 0.009\n",
      "[2,   560] loss: 0.012\n",
      "[2,   580] loss: 0.010\n",
      "[2,   600] loss: 0.010\n",
      "\n",
      "============\n",
      "epoch:  2\n",
      "[3,    20] loss: 0.009\n",
      "[3,    40] loss: 0.008\n",
      "[3,    60] loss: 0.009\n",
      "[3,    80] loss: 0.011\n",
      "[3,   100] loss: 0.010\n",
      "[3,   120] loss: 0.009\n",
      "[3,   140] loss: 0.010\n",
      "[3,   160] loss: 0.009\n",
      "[3,   180] loss: 0.008\n",
      "[3,   200] loss: 0.009\n",
      "[3,   220] loss: 0.010\n",
      "[3,   240] loss: 0.009\n",
      "[3,   260] loss: 0.009\n",
      "[3,   280] loss: 0.009\n",
      "[3,   300] loss: 0.010\n",
      "[3,   320] loss: 0.010\n",
      "[3,   340] loss: 0.009\n",
      "[3,   360] loss: 0.009\n",
      "[3,   380] loss: 0.009\n",
      "[3,   400] loss: 0.009\n",
      "[3,   420] loss: 0.009\n",
      "[3,   440] loss: 0.009\n",
      "[3,   460] loss: 0.009\n",
      "[3,   480] loss: 0.009\n",
      "[3,   500] loss: 0.010\n",
      "[3,   520] loss: 0.009\n",
      "[3,   540] loss: 0.011\n",
      "[3,   560] loss: 0.010\n",
      "[3,   580] loss: 0.009\n",
      "[3,   600] loss: 0.009\n",
      "\n",
      "============\n",
      "epoch:  3\n",
      "[4,    20] loss: 0.009\n",
      "[4,    40] loss: 0.009\n",
      "[4,    60] loss: 0.009\n",
      "[4,    80] loss: 0.008\n",
      "[4,   100] loss: 0.010\n",
      "[4,   120] loss: 0.009\n",
      "[4,   140] loss: 0.011\n",
      "[4,   160] loss: 0.010\n",
      "[4,   180] loss: 0.010\n",
      "[4,   200] loss: 0.010\n",
      "[4,   220] loss: 0.009\n",
      "[4,   240] loss: 0.009\n",
      "[4,   260] loss: 0.009\n",
      "[4,   280] loss: 0.009\n",
      "[4,   300] loss: 0.008\n",
      "[4,   320] loss: 0.009\n",
      "[4,   340] loss: 0.009\n",
      "[4,   360] loss: 0.009\n",
      "[4,   380] loss: 0.009\n",
      "[4,   400] loss: 0.008\n",
      "[4,   420] loss: 0.009\n",
      "[4,   440] loss: 0.008\n",
      "[4,   460] loss: 0.010\n",
      "[4,   480] loss: 0.009\n",
      "[4,   500] loss: 0.011\n",
      "[4,   520] loss: 0.009\n",
      "[4,   540] loss: 0.008\n",
      "[4,   560] loss: 0.008\n",
      "[4,   580] loss: 0.011\n",
      "[4,   600] loss: 0.009\n",
      "\n",
      "============\n",
      "epoch:  4\n",
      "[5,    20] loss: 0.010\n",
      "[5,    40] loss: 0.008\n",
      "[5,    60] loss: 0.007\n",
      "[5,    80] loss: 0.008\n",
      "[5,   100] loss: 0.009\n",
      "[5,   120] loss: 0.009\n",
      "[5,   140] loss: 0.007\n",
      "[5,   160] loss: 0.007\n",
      "[5,   180] loss: 0.008\n",
      "[5,   200] loss: 0.008\n",
      "[5,   220] loss: 0.010\n",
      "[5,   240] loss: 0.009\n",
      "[5,   260] loss: 0.009\n",
      "[5,   280] loss: 0.008\n",
      "[5,   300] loss: 0.009\n",
      "[5,   320] loss: 0.008\n",
      "[5,   340] loss: 0.009\n",
      "[5,   360] loss: 0.008\n",
      "[5,   380] loss: 0.008\n",
      "[5,   400] loss: 0.009\n",
      "[5,   420] loss: 0.009\n",
      "[5,   440] loss: 0.008\n",
      "[5,   460] loss: 0.010\n",
      "[5,   480] loss: 0.008\n",
      "[5,   500] loss: 0.009\n",
      "[5,   520] loss: 0.009\n",
      "[5,   540] loss: 0.009\n",
      "[5,   560] loss: 0.008\n",
      "[5,   580] loss: 0.008\n",
      "[5,   600] loss: 0.009\n",
      "\n",
      "============\n",
      "epoch:  5\n",
      "[6,    20] loss: 0.008\n",
      "[6,    40] loss: 0.008\n",
      "[6,    60] loss: 0.008\n",
      "[6,    80] loss: 0.007\n",
      "[6,   100] loss: 0.008\n",
      "[6,   120] loss: 0.007\n",
      "[6,   140] loss: 0.009\n",
      "[6,   160] loss: 0.009\n",
      "[6,   180] loss: 0.009\n",
      "[6,   200] loss: 0.008\n",
      "[6,   220] loss: 0.007\n",
      "[6,   240] loss: 0.008\n",
      "[6,   260] loss: 0.007\n",
      "[6,   280] loss: 0.009\n",
      "[6,   300] loss: 0.010\n",
      "[6,   320] loss: 0.007\n",
      "[6,   340] loss: 0.008\n",
      "[6,   360] loss: 0.009\n",
      "[6,   380] loss: 0.007\n",
      "[6,   400] loss: 0.009\n",
      "[6,   420] loss: 0.010\n",
      "[6,   440] loss: 0.008\n",
      "[6,   460] loss: 0.009\n",
      "[6,   480] loss: 0.008\n",
      "[6,   500] loss: 0.007\n",
      "[6,   520] loss: 0.008\n",
      "[6,   540] loss: 0.007\n",
      "[6,   560] loss: 0.009\n",
      "[6,   580] loss: 0.007\n",
      "[6,   600] loss: 0.008\n",
      "\n",
      "============\n",
      "epoch:  6\n",
      "[7,    20] loss: 0.008\n",
      "[7,    40] loss: 0.007\n",
      "[7,    60] loss: 0.007\n",
      "[7,    80] loss: 0.007\n",
      "[7,   100] loss: 0.009\n",
      "[7,   120] loss: 0.007\n",
      "[7,   140] loss: 0.007\n",
      "[7,   160] loss: 0.007\n",
      "[7,   180] loss: 0.006\n",
      "[7,   200] loss: 0.007\n",
      "[7,   220] loss: 0.008\n",
      "[7,   240] loss: 0.009\n",
      "[7,   260] loss: 0.007\n",
      "[7,   280] loss: 0.007\n",
      "[7,   300] loss: 0.006\n",
      "[7,   320] loss: 0.008\n",
      "[7,   340] loss: 0.008\n",
      "[7,   360] loss: 0.008\n",
      "[7,   380] loss: 0.007\n",
      "[7,   400] loss: 0.008\n",
      "[7,   420] loss: 0.008\n",
      "[7,   440] loss: 0.007\n",
      "[7,   460] loss: 0.007\n",
      "[7,   480] loss: 0.008\n",
      "[7,   500] loss: 0.009\n",
      "[7,   520] loss: 0.009\n",
      "[7,   540] loss: 0.008\n",
      "[7,   560] loss: 0.009\n",
      "[7,   580] loss: 0.008\n",
      "[7,   600] loss: 0.008\n",
      "\n",
      "============\n",
      "epoch:  7\n",
      "[8,    20] loss: 0.007\n",
      "[8,    40] loss: 0.007\n",
      "[8,    60] loss: 0.006\n",
      "[8,    80] loss: 0.008\n",
      "[8,   100] loss: 0.006\n",
      "[8,   120] loss: 0.006\n",
      "[8,   140] loss: 0.008\n",
      "[8,   160] loss: 0.008\n",
      "[8,   180] loss: 0.007\n",
      "[8,   200] loss: 0.008\n",
      "[8,   220] loss: 0.007\n",
      "[8,   240] loss: 0.007\n",
      "[8,   260] loss: 0.006\n",
      "[8,   280] loss: 0.005\n",
      "[8,   300] loss: 0.005\n",
      "[8,   320] loss: 0.007\n",
      "[8,   340] loss: 0.005\n",
      "[8,   360] loss: 0.007\n",
      "[8,   380] loss: 0.006\n",
      "[8,   400] loss: 0.005\n",
      "[8,   420] loss: 0.007\n",
      "[8,   440] loss: 0.007\n",
      "[8,   460] loss: 0.008\n",
      "[8,   480] loss: 0.007\n",
      "[8,   500] loss: 0.008\n",
      "[8,   520] loss: 0.007\n",
      "[8,   540] loss: 0.006\n",
      "[8,   560] loss: 0.005\n",
      "[8,   580] loss: 0.007\n",
      "[8,   600] loss: 0.007\n",
      "\n",
      "============\n",
      "epoch:  8\n",
      "[9,    20] loss: 0.006\n",
      "[9,    40] loss: 0.006\n",
      "[9,    60] loss: 0.004\n",
      "[9,    80] loss: 0.005\n",
      "[9,   100] loss: 0.008\n",
      "[9,   120] loss: 0.005\n",
      "[9,   140] loss: 0.005\n",
      "[9,   160] loss: 0.007\n",
      "[9,   180] loss: 0.006\n",
      "[9,   200] loss: 0.008\n",
      "[9,   220] loss: 0.007\n",
      "[9,   240] loss: 0.005\n",
      "[9,   260] loss: 0.007\n",
      "[9,   280] loss: 0.006\n",
      "[9,   300] loss: 0.004\n",
      "[9,   320] loss: 0.007\n",
      "[9,   340] loss: 0.006\n",
      "[9,   360] loss: 0.005\n",
      "[9,   380] loss: 0.007\n",
      "[9,   400] loss: 0.008\n",
      "[9,   420] loss: 0.006\n",
      "[9,   440] loss: 0.006\n",
      "[9,   460] loss: 0.006\n",
      "[9,   480] loss: 0.006\n",
      "[9,   500] loss: 0.006\n",
      "[9,   520] loss: 0.006\n",
      "[9,   540] loss: 0.005\n",
      "[9,   560] loss: 0.007\n",
      "[9,   580] loss: 0.006\n",
      "[9,   600] loss: 0.005\n",
      "\n",
      "============\n",
      "epoch:  9\n",
      "[10,    20] loss: 0.003\n",
      "[10,    40] loss: 0.003\n",
      "[10,    60] loss: 0.002\n",
      "[10,    80] loss: 0.004\n",
      "[10,   100] loss: 0.007\n",
      "[10,   120] loss: 0.006\n",
      "[10,   140] loss: 0.006\n",
      "[10,   160] loss: 0.004\n",
      "[10,   180] loss: 0.003\n",
      "[10,   200] loss: 0.005\n",
      "[10,   220] loss: 0.004\n",
      "[10,   240] loss: 0.003\n",
      "[10,   260] loss: 0.004\n",
      "[10,   280] loss: 0.006\n",
      "[10,   300] loss: 0.005\n",
      "[10,   320] loss: 0.003\n",
      "[10,   340] loss: 0.006\n",
      "[10,   360] loss: 0.004\n",
      "[10,   380] loss: 0.005\n",
      "[10,   400] loss: 0.004\n",
      "[10,   420] loss: 0.004\n",
      "[10,   440] loss: 0.004\n",
      "[10,   460] loss: 0.005\n",
      "[10,   480] loss: 0.006\n",
      "[10,   500] loss: 0.006\n",
      "[10,   520] loss: 0.003\n",
      "[10,   540] loss: 0.003\n",
      "[10,   560] loss: 0.005\n",
      "[10,   580] loss: 0.006\n",
      "[10,   600] loss: 0.006\n",
      "Finished Training\n",
      "/home/chad/222 AIML421/Assignments/Project/code/model.pth\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72e96d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:   ('cherry', 'strawberry', 'tomato')\n",
      "load PATH =  /home/chad/222 AIML421/Assignments/Project/code/model.pth\n",
      "Network model loaded\n",
      "Accuracy of the network on the test images: 50 %\n",
      "Accuracy for class: cherry is 66.3 %\n",
      "Accuracy for class: strawberry is 34.9 %\n",
      "Accuracy for class: tomato is 50.4 %\n"
     ]
    }
   ],
   "source": [
    "!python3 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e054246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
